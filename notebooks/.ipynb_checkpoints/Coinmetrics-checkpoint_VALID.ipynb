{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdblp as bbg\n",
    "from sklearn import preprocessing as pp\n",
    "import sklearn as skl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import yellowbrick\n",
    "import requests\n",
    "import json\n",
    "print(tf.__version__)\n",
    "from yellowbrick.features import RadViz, Rank2D, Rank1D, ParallelCoordinates\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "#from sklearn. import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "%matplotlib inline\n",
    "#relevant_features = ['AdrActCnt', 'CapAct1yrUSD', 'CapRealUSD', 'CapMVRVCur', \n",
    "#                        'HashRate', 'SplyAct1yr', 'NVTAdj', 'NVTAdj90','TxCnt', 'TxTfr','TxTfrValAdjUSD', \n",
    "#                        'TxTfrValUSD', 'TxTfrValDayDst', 'TxTfrValDayDstMean' ,'UTXOCnt', 'VelActAdj1yr', 'VelCurAdj1yr', 'PriceUSD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\skennedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "c:\\users\\skennedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "c:\\users\\skennedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "btc\n",
      "AdrActCnt\n",
      "CapAct1yrUSD\n",
      "CapRealUSD\n",
      "CapMVRVCur\n",
      "HashRate\n",
      "SplyAct1yr\n",
      "NVTAdj\n",
      "NVTAdj90\n",
      "TxCnt\n",
      "TxTfr\n",
      "TxTfrValAdjUSD\n",
      "TxTfrValUSD\n",
      "TxTfrValDayDst\n",
      "TxTfrValDayDstMean\n",
      "UTXOCnt\n",
      "VelActAdj1yr\n",
      "VelCurAdj1yr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\skennedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\data.py:180: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "c:\\users\\skennedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\data.py:180: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "c:\\users\\skennedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\data.py:180: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Linear Regression Results:\n",
      "                    Scoring Metric: r2\n",
      "                    Train Score = 0.08548771476956063\n",
      "                    Test  Score = -0.08677533040690232\n",
      "                    MSE = -1.322752747600323\n",
      "                    Intercept = [0.37533829]\n",
      "          \n",
      "                         feature  coefficient  absCoefficient\n",
      "11          CapMVRVCur_1_DAY_LAG    -2.490649        2.490649\n",
      "5         CapAct1yrUSD_1_DAY_LAG     2.080457        2.080457\n",
      "8           CapRealUSD_1_DAY_LAG    -1.077330        1.077330\n",
      "23            NVTAdj90_1_DAY_LAG     0.601600        0.601600\n",
      "4            CapAct1yrUSD_SCALED     0.545942        0.545942\n",
      "24            NVTAdj90_2_DAY_LAG    -0.443352        0.443352\n",
      "12          CapMVRVCur_2_DAY_LAG     0.338579        0.338579\n",
      "10             CapMVRVCur_SCALED     0.291996        0.291996\n",
      "43                UTXOCnt_SCALED    -0.196939        0.196939\n",
      "1               AdrActCnt_SCALED     0.183809        0.183809\n",
      "13               HashRate_SCALED     0.159040        0.159040\n",
      "9           CapRealUSD_2_DAY_LAG    -0.115546        0.115546\n",
      "34            TxTfrValUSD_SCALED     0.105420        0.105420\n",
      "49           VelCurAdj1yr_SCALED    -0.096062        0.096062\n",
      "6         CapAct1yrUSD_2_DAY_LAG     0.093610        0.093610\n",
      "19                 NVTAdj_SCALED    -0.092647        0.092647\n",
      "14            HashRate_1_DAY_LAG     0.081642        0.081642\n",
      "32      TxTfrValAdjUSD_1_DAY_LAG     0.076376        0.076376\n",
      "46           VelActAdj1yr_SCALED     0.070853        0.070853\n",
      "2            AdrActCnt_1_DAY_LAG    -0.064511        0.064511\n",
      "16             SplyAct1yr_SCALED     0.060082        0.060082\n",
      "17          SplyAct1yr_1_DAY_LAG    -0.059260        0.059260\n",
      "50        VelCurAdj1yr_1_DAY_LAG     0.056331        0.056331\n",
      "37         TxTfrValDayDst_SCALED    -0.053268        0.053268\n",
      "25                  TxCnt_SCALED    -0.045118        0.045118\n",
      "29               TxTfr_1_DAY_LAG     0.043393        0.043393\n",
      "27               TxCnt_2_DAY_LAG    -0.036587        0.036587\n",
      "7              CapRealUSD_SCALED    -0.030454        0.030454\n",
      "33      TxTfrValAdjUSD_2_DAY_LAG     0.026918        0.026918\n",
      "30               TxTfr_2_DAY_LAG     0.024988        0.024988\n",
      "35         TxTfrValUSD_1_DAY_LAG    -0.020903        0.020903\n",
      "26               TxCnt_1_DAY_LAG    -0.018535        0.018535\n",
      "47        VelActAdj1yr_1_DAY_LAG    -0.018431        0.018431\n",
      "20              NVTAdj_1_DAY_LAG     0.015372        0.015372\n",
      "51        VelCurAdj1yr_2_DAY_LAG     0.013907        0.013907\n",
      "36         TxTfrValUSD_2_DAY_LAG    -0.012312        0.012312\n",
      "41  TxTfrValDayDstMean_1_DAY_LAG     0.011404        0.011404\n",
      "28                  TxTfr_SCALED     0.009926        0.009926\n",
      "22               NVTAdj90_SCALED     0.009374        0.009374\n",
      "15            HashRate_2_DAY_LAG    -0.007319        0.007319\n",
      "48        VelActAdj1yr_2_DAY_LAG     0.006529        0.006529\n",
      "38      TxTfrValDayDst_1_DAY_LAG    -0.006455        0.006455\n",
      "18          SplyAct1yr_2_DAY_LAG    -0.006229        0.006229\n",
      "3            AdrActCnt_2_DAY_LAG     0.005999        0.005999\n",
      "21              NVTAdj_2_DAY_LAG    -0.005523        0.005523\n",
      "31         TxTfrValAdjUSD_SCALED     0.005287        0.005287\n",
      "40     TxTfrValDayDstMean_SCALED     0.005265        0.005265\n",
      "44             UTXOCnt_1_DAY_LAG    -0.004454        0.004454\n",
      "45             UTXOCnt_2_DAY_LAG     0.001440        0.001440\n",
      "39      TxTfrValDayDst_2_DAY_LAG    -0.000530        0.000530\n",
      "0                  NextDayReturn    -0.000270        0.000270\n",
      "42  TxTfrValDayDstMean_2_DAY_LAG    -0.000079        0.000079\n",
      "feature            object\n",
      "coefficient       float64\n",
      "absCoefficient    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "relevant_features = ['AdrActCnt', 'CapAct1yrUSD', 'CapRealUSD', 'CapMVRVCur', \n",
    "                        'HashRate', 'SplyAct1yr', 'NVTAdj', 'NVTAdj90','TxCnt', 'TxTfr','TxTfrValAdjUSD', \n",
    "                        'TxTfrValUSD', 'TxTfrValDayDst', 'TxTfrValDayDstMean' ,'UTXOCnt', 'VelActAdj1yr', 'VelCurAdj1yr', 'PriceUSD']\n",
    "df = pd.DataFrame(get_coin_metrics_assets())\n",
    "assets = list(df.assets.values)\n",
    "for a in ['btc']: #assets[0:1]:\n",
    "    asset_data = pd.DataFrame(get_coin_metrics_asset_info(a))\n",
    "    for x in asset_data[0:1]:\n",
    "        metrics = pd.DataFrame(get_coin_metrics_data(asset_data[x].id, ','.join(relevant_features))).reset_index()\n",
    "        print(asset_data[x].id)\n",
    "        time_series = pd.DataFrame(columns=metrics[metrics['index']=='metrics']['metricData'][0])\n",
    "        x = pd.DataFrame(metrics[metrics['index']=='series'])['metricData'].values\n",
    "        times = []\n",
    "        for z in [k for k in x[0]]:\n",
    "            for y,v in z.items():\n",
    "                if isinstance(v, str):\n",
    "                    times.append(v)\n",
    "                else:\n",
    "                    time_series.loc[len(time_series)] = v\n",
    "                    \n",
    "        time_series['times'] = times\n",
    "        time_series['OneDayReturn'] = time_series.PriceUSD.astype('float').pct_change(1)\n",
    "        time_series.loc[0, 'NextDayReturn'] =  time_series.loc[0, 'OneDayReturn']\n",
    "        for i in time_series.index:\n",
    "            try:\n",
    "                time_series.loc[i, 'NextDayReturn'] = time_series.loc[i+1, 'OneDayReturn']\n",
    "            except:\n",
    "                time_series.loc[i, 'NextDayReturn'] = 0\n",
    "        \n",
    "        time_series.to_csv(f'{a}.csv')\n",
    "        \n",
    "        time_series['NextDayReturn'] = scale(pp.scale, time_series['NextDayReturn'])\n",
    "        relevant_features.remove('PriceUSD')\n",
    "        \n",
    "        time_series.replace([np.nan, np.inf, -np.inf], 0, inplace=True)\n",
    "        \n",
    "        for feature in relevant_features: #scale and lag all but priceUSD \n",
    "            time_series[feature] = time_series[feature].astype('float64')\n",
    "            if time_series[feature].dtype == np.dtype('float64'):\n",
    "                print(feature)\n",
    "                time_series[feature + '_SCALED'] = scale(pp.scale, time_series[feature])\n",
    "                for lag in range(1,3):\n",
    "                    time_series[feature + f'_{lag}_DAY_LAG'] = scale(pp.scale, time_series[feature].pct_change(lag).replace([np.nan, np.inf, -np.inf], 0))\n",
    "    relevant_features.append('times')\n",
    "    relevant_features.append('OneDayReturn')\n",
    "    final_data = time_series.drop(relevant_features, axis=1)\n",
    "    #print(final_data.describe())\n",
    "    scoring_metric = 'r2'\n",
    "    [errors, linear_model, train_score, test_score, intercept, coeffs] = run_linear_regression(final_data, 'NextDayReturn', score=scoring_metric)\n",
    "    print(f'''\n",
    "                Linear Regression Results:\n",
    "                    Scoring Metric: {scoring_metric}\n",
    "                    Train Score = {train_score}\n",
    "                    Test  Score = {test_score}\n",
    "                    MSE = {errors.mean()}\n",
    "                    Intercept = {intercept}\n",
    "          ''')\n",
    "    coeffs = pd.DataFrame(coeffs, columns=final_data.columns[1:]).T.reset_index()\n",
    "    coeffs.rename(columns={'index':'feature', 0:'coefficient'}, inplace=True)\n",
    "    coeffs['absCoefficient'] = np.abs(coeffs['coefficient']) \n",
    "    \n",
    "    coeffs.sort_values(by=['absCoefficient'], ascending=False, inplace=True)\n",
    "    print(coeffs.head(100))\n",
    "    print(coeffs.dtypes)\n",
    "    #cointegration - mean reversion\n",
    "    #use PriceBTC for other coins (eth for other ERC20?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scatter_plot(data, feature, target):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.scatter(data[feature], \n",
    "                data[target],\n",
    "                c='black'\n",
    "    )\n",
    "    plt.xlabel(f'Feature = {feature}')\n",
    "    plt.ylabel(f'Target = {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_coin_metrics_assets():\n",
    "    coin_metrics_session = requests.Session()\n",
    "    coin_metrics_url = 'https://api.coinmetrics.io/v2/assets?'\n",
    "    coin_metrics_api_key = '&api_key=DlgTDcpDmRS8H5gNnila' #append to every request\n",
    "    response = coin_metrics_session.get(coin_metrics_url + coin_metrics_api_key, verify=False)\n",
    "    return json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_coin_metrics_asset_info(asset_id):\n",
    "    coin_metrics_session = requests.Session()\n",
    "    coin_metrics_url = f'https://api.coinmetrics.io/v2/assets/{asset_id}'\n",
    "    coin_metrics_api_key = '?api_key=DlgTDcpDmRS8H5gNnila' #append to every request\n",
    "    #print(coin_metrics_url + coin_metrics_api_key)\n",
    "    response = coin_metrics_session.get(coin_metrics_url + coin_metrics_api_key, verify=False)\n",
    "    return json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_coin_metrics_metrics():\n",
    "    coin_metrics_session = requests.Session()\n",
    "    coin_metrics_url = 'https://api.coinmetrics.io/v2/metric?'\n",
    "    coin_metrics_api_key = '&api_key=DlgTDcpDmRS8H5gNnila' #append to every request\n",
    "    response = coin_metrics_session.get(coin_metrics_url + coin_metrics_api_key, verify=False)\n",
    "    return json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_coin_metrics_data(asset_id, metrics):\n",
    "    coin_metrics_session = requests.Session()\n",
    "    coin_metrics_url = f'https://api.coinmetrics.io/v2/assets/{asset_id}/metricdata?'\n",
    "    coin_metrics_api_key = 'api_key=DlgTDcpDmRS8H5gNnila' #append to every request\n",
    "    query_string = f'&metrics={metrics}'\n",
    "    response = coin_metrics_session.get(coin_metrics_url + coin_metrics_api_key + query_string, verify=False)\n",
    "    return json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encode(encoder, data, reshape=False):\n",
    "    if reshape:\n",
    "        data = np.array(data).reshape(-1,1) #do not reshape for label encoding, reshape for to 1D array for OHE\n",
    "    return encoder.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scale(scaler, data):\n",
    "    return scaler(data, with_std=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    data = np.array(data).reshape(-1,1)\n",
    "    return pp.Normalizer().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_linear_regression(data, target, score='neg_mean_squared_error'):\n",
    "    Xs = data.drop([target], axis=1)\n",
    "    y = data[target].values.reshape(-1, 1)\n",
    "    #train test split\n",
    "    X_train, X_test, y_train, y_test =train_test_split(Xs,y, test_size=0.3, random_state=3)\n",
    "    \n",
    "    #fit to training set\n",
    "    lin_reg = LinearRegression().fit(X_train, y_train)\n",
    "    \n",
    "    train_score = lin_reg.score(X_train,y_train)\n",
    "    test_score = lin_reg.score(X_test,y_test)\n",
    "    MSEs = cross_val_score(lin_reg, X_train, y_train, scoring=score, cv=5)\n",
    "    #lin_reg = lin_reg.fit(Xs, y)\n",
    "    return MSEs, lin_reg, train_score, test_score, lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_ridge_regression(data, target, alphas={'alpha':[1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]}):\n",
    "    Xs = data.drop([target], axis=1)\n",
    "    y = data[target].values.reshape(-1, 1)\n",
    "    ridge = Ridge()\n",
    "    ridge_regression = GridSearchCV(ridge, alphas, scoring='neg_mean_squared_error', cv=5)\n",
    "    ridge_regression.fit(Xs, y)\n",
    "    print(ridge_regression.best_params_)\n",
    "    print(ridge_regression.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_lasso_regression(data, target, alphas={'alpha':[1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]}):\n",
    "    Xs = data.drop([target], axis=1)\n",
    "    y = data[target].values.reshape(-1, 1)\n",
    "    lasso = Lasso()\n",
    "    lasso_regression = GridSearchCV(lasso, alphas, scoring='neg_mean_squared_error', cv=5)\n",
    "    lasso_regression.fit(Xs, y)\n",
    "    print(lasso_regression.best_params_)\n",
    "    print(lasso_regression.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_sub_plot(data, rows = None, cols = None):\n",
    "    if not cols:\n",
    "        cols = 3\n",
    "    if not rows:\n",
    "        rows = int(round(len(data.columns)/cols,0))\n",
    "    fig, ax = plt.subplots(rows, cols)\n",
    "    ax = ax.ravel()\n",
    "    i = 0\n",
    "    for col in data.columns:\n",
    "        ax[i].hist(data[col], bins='auto', alpha=0.5)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\skennedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "c:\\users\\skennedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "btc\n",
      "     AdrActCnt AdrActRecCnt AdrActSentCnt AdrBalAdjCnt AdrBalCnt BlkCnt  \\\n",
      "2554    316781       264604        198240      1972087   7058031    135   \n",
      "\n",
      "            BlkIntMean   BlkSizeMeanByte        CapAct1yrUSD  \\\n",
      "2554  640.544776119403  492924.844444444  2.58247300407995e9   \n",
      "\n",
      "      CapFutExp10yrUSD  ... TxTfrValMedNtv    TxTfrValMedUSD      TxTfrValNtv  \\\n",
      "2554  8.703467804504e9  ...        1.65e-2  7.20634657406522  949127.28463957   \n",
      "\n",
      "             TxTfrValUSD   UTXOCnt         VelAct1yr      VelActAdj1yr  \\\n",
      "2554  4.14529706425102e8  33378920  92.5089586118135  19.2786119817209   \n",
      "\n",
      "             VelCur1yr      VelCurAdj1yr                     times  \n",
      "2554  36.3869801536448  7.58294636644108 2016-01-01 00:00:00+00:00  \n",
      "\n",
      "[1 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "#relevant_features = ['AdrActCnt', 'CapAct1yrUSD', 'CapRealUSD', 'CapMVRVCur', \n",
    "#                        'HashRate', 'SplyAct1yr', 'NVTAdj', 'NVTAdj90','TxCnt', 'TxTfr','TxTfrValAdjUSD', \n",
    "#                        'TxTfrValUSD', 'TxTfrValDayDst', 'TxTfrValDayDstMean' ,'UTXOCnt', 'VelActAdj1yr', 'VelCurAdj1yr', 'PriceUSD']\n",
    "for a in ['btc']: #assets[0:1]:\n",
    "    asset_data = pd.DataFrame(get_coin_metrics_asset_info(a))\n",
    "    for x in asset_data[0:1]:\n",
    "        metrics = pd.DataFrame(get_coin_metrics_data(asset_data[x].id, ','.join(list(asset_data[x].metrics)))).reset_index()\n",
    "        print(asset_data[x].id)\n",
    "        time_series = pd.DataFrame(columns=metrics[metrics['index']=='metrics']['metricData'][0])\n",
    "        x = pd.DataFrame(metrics[metrics['index']=='series'])['metricData'].values\n",
    "        times = []\n",
    "        for z in [k for k in x[0]]:\n",
    "            for y,v in z.items():\n",
    "                if isinstance(v, str):\n",
    "                    times.append(pd.to_datetime(v))\n",
    "                else:\n",
    "                    time_series.loc[len(time_series)] = v\n",
    "        \n",
    "        times = pd.Series(times)\n",
    "        time_series['times'] = times\n",
    "        min_data_by_year = time_series.groupby(time_series.times.dt.year).min()\n",
    "        #print(min_data_by_year.T.head(10))\n",
    "        min_data_by_year.T.to_csv(f'{a}.csv')\n",
    "        print(time_series[time_series.times.dt.year == 2016].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
